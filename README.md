# MORL-with-Policy-Orchestration

TO DO: 
 - [ ] In src/orchestrator.py add the value functions terms to the total_reward in self.update_beliefs(). You can approx it to the first order;
 - [ ] Test src/Q_learner.py with the ram-vector;
 - [ ] Build a policy by using Q_learner changing the reward for eating a ghost to negative. It will be the expert policy;
 - [ ] Collect a set of trajectories by means of the expert policy;
 - [ ] Add a way to store weights to each learning algorithm in src/;
 - [ ] Build the inverse_learner class;